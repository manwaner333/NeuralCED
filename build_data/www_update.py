import os
import json
import logging
from openai import OpenAI, AzureOpenAI
from tqdm import tqdm
import re
import numpy as np
from resource import openai_key
import csv

logging.getLogger("openai").setLevel(logging.ERROR)
logging.getLogger("httpx").setLevel(logging.ERROR)

# sys_prompt = """ You are a helpful assistant. You will receive a piece of original text and text rewritten by AI. Please act as an impartial judge and Based on the original text, determine whether the data generated by AI contains hallucination.
#
# Please evaluate the AI assistant's response using the following scale:
# - Rating: [[2]]: The response generated by AI don't contains hallucination.
# - Rating: [[1]]: The response generated by AI contains hallucination.
#
# Your rating should strictly follow this format: "Rating: [[rating]]", the rating in the double-closed brackets is a number from {1, 2}.
# """

sys_prompt = """ You are a helpful and impartial assistant. You will receive an original text and a version rewritten by AI. Your task is to evaluate whether the AI-generated text contains hallucinations based on the original text.

Please use the following scale to rate your evaluation:
- Rating: [[2]]: The AI-generated text does not contain hallucinations.
- Rating: [[1]]: The AI-generated text contains hallucinations.

Provide your rating strictly in this format: "Rating: [[rating]]", where the rating inside the double brackets must be either 1 or 2.
"""


def call_llm_api(messages, max_tokens, temperature, top_p, n, stop, engine="gpt-4o"):
    client = OpenAI(api_key=openai_key)
    result = client.chat.completions.create(
        model=engine,
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        n=n,
        stop=stop,
        seed=0,
    )

    return result


def gpt_wrapper(qa, sys_prompt=None, max_tokens=512):
    if sys_prompt is None:
        messages = []
    else:
        messages = [{
            "role": "system",
            "content": sys_prompt
        }]
    messages.append(
        {
            "role": "user",
            "content": qa
        }
    )
    try:
        result = call_llm_api(messages, max_tokens, temperature=0.0, top_p=1.0, n=1, stop=["\n\n"])
    except Exception as e:
        print(e)
        return None, str(e)
    raw = result.choices[0].message.content
    score = re.findall(r': \[\[(\d)\]\]', raw)
    if len(score) != 1:
        print(f"Error: {raw}")
        return None, raw
    return int(score[0]), raw


def evaluate_gpt():
    dataset = [{"Original": "Nobel prize in literature to be announced http:\/\/t.co\/qxlEqdl3",
               "Add_context": "Stay tuned for the exciting announcement of the Nobel Prize in Literature, which will take place shortly. You can follow the event live here: http:\/\/t.co\/qxlEqdl3."},
               {"Original": "\u201c@marvicleonen: Is it true that UP won UAAP basketball?\u201d -- Next year, Dean. Sure na 'yan!",
                "Add_context": "Aren't we all excited about the performance of UP in the UAAP basketball finals? I heard they clinched the title this year! What do you think, Dean? It's a sure win!"}
               ]
    result = []
    for item in tqdm(dataset):
        original_text = item['Original']
        text_by_AI = item['Add_context']
        # hallucination avoidance
        qa = f"Original Text: {original_text}\nAI-generated Text: {text_by_AI}"
        input_prompt = f"{sys_prompt}\n\nNow classify the following response:\n{qa}"
        pred, raw = gpt_wrapper(input_prompt, sys_prompt=None)
        result.append(pred)
    return result



if __name__ == '__main__':
    print("###############################")
    print("GPT evaluations")
    print("###############################")
    result = evaluate_gpt()
    print(result)
